{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10714717,"sourceType":"datasetVersion","datasetId":6641377},{"sourceId":173790,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":147934,"modelId":170434},{"sourceId":173805,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":147946,"modelId":170434}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:42:46.206783Z","iopub.execute_input":"2025-02-11T09:42:46.207060Z","iopub.status.idle":"2025-02-11T09:42:46.540037Z","shell.execute_reply.started":"2025-02-11T09:42:46.207036Z","shell.execute_reply":"2025-02-11T09:42:46.539338Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/qwen2-vl/transformers/2b-instruct/1/model.safetensors.index.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/config.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/merges.txt\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/model-00001-of-00002.safetensors\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/LICENSE\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/model-00002-of-00002.safetensors\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/preprocessor_config.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/README.md\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/tokenizer.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/vocab.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/tokenizer_config.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/chat_template.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/.gitattributes\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/generation_config.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model.safetensors.index.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/config.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/merges.txt\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/LICENSE\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/preprocessor_config.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00005-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00001-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/README.md\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00002-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/tokenizer.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/vocab.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/tokenizer_config.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/chat_template.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00004-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00003-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/.gitattributes\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/generation_config.json\n/kaggle/input/data100/Retail Store Outlet.mp4\n/kaggle/input/data100/Two Wheeler Safety.mp4\n/kaggle/input/data100/Fire Outbreak.mp4\n/kaggle/input/data100/Garbage.mp4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate\n!pip install -q optimum auto-gptq qwen-vl-utils[decord]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:42:46.541045Z","iopub.execute_input":"2025-02-11T09:42:46.541450Z","iopub.status.idle":"2025-02-11T09:43:30.457125Z","shell.execute_reply.started":"2025-02-11T09:42:46.541426Z","shell.execute_reply":"2025-02-11T09:43:30.456127Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:44:11.110022Z","iopub.execute_input":"2025-02-11T09:44:11.110316Z","iopub.status.idle":"2025-02-11T09:44:14.637706Z","shell.execute_reply.started":"2025-02-11T09:44:11.110293Z","shell.execute_reply":"2025-02-11T09:44:14.636736Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.15.0)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.8)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.5.0)\nRequirement already satisfied: gradio-client==1.7.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.7.0)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.11.0a1)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.6)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.45.3)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.2)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.34.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (2024.9.0)\nRequirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (14.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.28.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import gradio as gr\nimport torch\nimport cv2\nfrom PIL import Image\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n\nclass VideoAnalyzer:\n    def __init__(self, model_size=\"2b\"):\n        \"\"\"Initialize with specified model size (2b, 7b, or 72b)\"\"\"\n        model_paths = {\n            \"2b\": \"/kaggle/input/qwen2-vl/transformers/2b-instruct/1\",\n            \"7b\": \"/kaggle/input/qwen2-vl/transformers/7b-instruct/1\",\n            \"72b\": \"/kaggle/input/qwen2-vl/transformers/72b-instruct/1\"\n        }\n        \n        model_path = model_paths[model_size]\n        print(f\"Loading model from: {model_path}\")\n        \n        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n            model_path,\n            torch_dtype=\"auto\",\n            device_map=\"auto\"\n        )\n        self.processor = AutoProcessor.from_pretrained(model_path)\n\n    def analyze_frame(self, frame, prompt):\n        \"\"\"Analyze a single frame with given prompt\"\"\"\n        if not isinstance(frame, Image.Image):\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        \n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"image\": frame,\n                    },\n                    {\"type\": \"text\", \"text\": prompt},\n                ],\n            }\n        ]\n        \n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        generated_ids = self.model.generate(**inputs, max_new_tokens=128)\n        generated_ids_trimmed = [\n            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        output_text = self.processor.batch_decode(\n            generated_ids_trimmed, \n            skip_special_tokens=True, \n            clean_up_tokenization_spaces=False\n        )\n        \n        return output_text[0]\n\n    def process_video(self, video_path, prompt, frame_interval=30):\n        \"\"\"Process video with adaptive analysis based on prompt type\"\"\"\n        # Detect prompt type\n        needs_timestamps = any(keyword in prompt.lower() for keyword in \n                             ['timestamp', 'when', 'time', 'moment', 'detect'])\n        \n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(\"Error opening video file\")\n        \n        frames = []\n        timestamps = []\n        frame_analyses = []\n        frame_count = 0\n        \n        print(\"Processing video frames...\")\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            if frame_count % frame_interval == 0:\n                timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n                frames.append(frame)\n                timestamps.append(timestamp)\n                print(f\"Captured frame at {timestamp:.2f}s\")\n            \n            frame_count += 1\n        \n        cap.release()\n        \n        # Process frames\n        for frame, timestamp in zip(frames, timestamps):\n            analysis = self.analyze_frame(frame, prompt)\n            frame_analyses.append({\n                'timestamp': timestamp,\n                'analysis': analysis\n            })\n            print(f\"Processed frame at {timestamp:.2f}s\")\n        \n        # Generate final output based on prompt type\n        if needs_timestamps:\n            # Return individual frame analyses for timestamp-specific queries\n            return frame_analyses\n        else:\n            # Aggregate analysis for general summaries\n            all_observations = [analysis['analysis'] for analysis in frame_analyses]\n            summary_prompt = f\"Based on these observations from different timestamps of the video, provide a comprehensive summary: {all_observations}\"\n            \n            # Use the first frame as a reference frame for the summary\n            if frames:\n                final_summary = self.analyze_frame(frames[0], summary_prompt)\n                return [{\n                    'timestamp': 'Full Video',\n                    'analysis': final_summary\n                }]\n        \n        return []\n\ndef create_video_analysis_interface():\n    \"\"\"Create Gradio interface for video analysis\"\"\"\n    # Function to manage model initialization\n    def initialize_analyzer(model_size):\n        \"\"\"Initialize analyzer with selected model size\"\"\"\n        return VideoAnalyzer(model_size=model_size)\n    \n    # Function to process video or webcam input\n    def analyze_video(video, prompt, model_size):\n        \"\"\"Process video and format results\"\"\"\n        try:\n            # Initialize analyzer with selected model\n            analyzer = initialize_analyzer(model_size)\n            \n            # Process the video\n            results = analyzer.process_video(video, prompt)\n            \n            # Format results for display\n            output_text = \"\"\n            for result in results:\n                if result['timestamp'] == 'Full Video':\n                    output_text += \"Complete Video Analysis:\\n\"\n                else:\n                    output_text += f\"Timestamp: {result['timestamp']:.2f}s\\n\"\n                output_text += f\"Analysis: {result['analysis']}\\n\"\n                output_text += \"-\" * 50 + \"\\n\"\n            \n            return output_text\n        \n        except Exception as e:\n            return f\"An error occurred: {str(e)}\"\n    \n    # Create Gradio interface with multiple inputs\n    interface = gr.Interface(\n        fn=analyze_video,\n        inputs=[\n            gr.Video(label=\"Upload Video\"),\n            gr.Textbox(label=\"Analysis Prompt\"),\n            gr.Dropdown(\n                choices=[\"2b\", \"7b\", \"72b\"], \n                value=\"2b\", \n                label=\"Select Model Size\"\n            )\n        ],\n        outputs=gr.Textbox(label=\"Analysis Results\"),\n        title=\"Video Analysis with Qwen2-VL\",\n        description=\"Upload a video, choose a model, and provide a specific prompt for analysis!\",\n        examples=[\n            [None, \"Describe what's happening in this video\", \"2b\"],\n            [None, \"Count the number of people in the scene\", \"7b\"],\n            [None, \"Detect when a person enters or leaves the frame\", \"72b\"]\n        ]\n    )\n    \n    return interface\n\n# Launch the interface\ndef main():\n    # Check GPU availability\n    print(\"GPU available:\", torch.cuda.is_available())\n    if torch.cuda.is_available():\n        print(\"GPU name:\", torch.cuda.get_device_name(0))\n    \n    # Create and launch the interface\n    interface = create_video_analysis_interface()\n    interface.launch(share=True, pwa=True)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T09:44:21.305059Z","iopub.execute_input":"2025-02-11T09:44:21.305368Z","iopub.status.idle":"2025-02-11T09:44:44.474389Z","shell.execute_reply.started":"2025-02-11T09:44:21.305343Z","shell.execute_reply":"2025-02-11T09:44:44.473751Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nGPU name: Tesla P100-PCIE-16GB\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://774abf0c5bb2592588.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://774abf0c5bb2592588.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}