{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10700846,"sourceType":"datasetVersion","datasetId":6631411},{"sourceId":173787,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":147931,"modelId":170434},{"sourceId":173790,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":147934,"modelId":170434},{"sourceId":173805,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":147946,"modelId":170434}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T08:57:10.442641Z","iopub.execute_input":"2025-02-10T08:57:10.442959Z","iopub.status.idle":"2025-02-10T08:57:10.786433Z","shell.execute_reply.started":"2025-02-10T08:57:10.442934Z","shell.execute_reply":"2025-02-10T08:57:10.785322Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mydata/Retail Store Outlet.mp4\n/kaggle/input/mydata/Two Wheeler Safety.mp4\n/kaggle/input/mydata/Fire Outbreak.mp4\n/kaggle/input/mydata/Garbage.mp4\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model.safetensors.index.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/config.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/merges.txt\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/LICENSE\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/preprocessor_config.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00005-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00001-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/README.md\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00002-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/tokenizer.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/vocab.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/tokenizer_config.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/chat_template.json\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00004-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/model-00003-of-00005.safetensors\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/.gitattributes\n/kaggle/input/qwen2-vl/transformers/7b-instruct/1/generation_config.json\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model.safetensors.index.json\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00006-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00016-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00038-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00002-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00035-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00030-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/config.json\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00032-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00028-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00023-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/merges.txt\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00036-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00018-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/LICENSE\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00001-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00005-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/preprocessor_config.json\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00004-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00013-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00025-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00017-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/README.md\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00019-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00037-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00033-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/tokenizer.json\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/vocab.json\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00020-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/tokenizer_config.json\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00007-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00029-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00034-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00010-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/chat_template.json\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00026-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00012-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00015-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00022-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00014-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00027-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00031-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00024-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00021-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00003-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00011-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00008-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/model-00009-of-00038.safetensors\n/kaggle/input/qwen2-vl/transformers/72b-instruct/1/generation_config.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/model.safetensors.index.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/config.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/merges.txt\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/model-00001-of-00002.safetensors\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/LICENSE\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/model-00002-of-00002.safetensors\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/preprocessor_config.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/README.md\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/tokenizer.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/vocab.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/tokenizer_config.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/chat_template.json\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/.gitattributes\n/kaggle/input/qwen2-vl/transformers/2b-instruct/1/generation_config.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830 accelerate\n!pip install -q optimum auto-gptq qwen-vl-utils[decord]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T03:58:18.370989Z","iopub.execute_input":"2025-02-11T03:58:18.371253Z","iopub.status.idle":"2025-02-11T03:59:03.596959Z","shell.execute_reply.started":"2025-02-11T03:58:18.371232Z","shell.execute_reply":"2025-02-11T03:59:03.596091Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom qwen_vl_utils import process_vision_info\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nimport torch\nfrom collections import defaultdict\n\nclass VideoAnalyzer:\n    def __init__(self, model_size=\"2b\"):\n        \"\"\"Initialize with specified model size (2b, 7b, or 72b)\"\"\"\n        model_paths = {\n            \"2b\": \"/kaggle/input/qwen2-vl/transformers/2b-instruct/1\",\n            \"7b\": \"/kaggle/input/qwen2-vl/transformers/7b-instruct/1\",\n            \"72b\": \"/kaggle/input/qwen2-vl/transformers/72b-instruct/1\"\n        }\n        \n        model_path = model_paths[model_size]\n        print(f\"Loading model from: {model_path}\")\n        \n        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n            model_path,\n            torch_dtype=\"auto\",\n            device_map=\"auto\"\n        )\n        self.processor = AutoProcessor.from_pretrained(model_path)\n\n    def analyze_frame(self, frame, prompt):\n        \"\"\"Analyze a single frame with given prompt\"\"\"\n        if not isinstance(frame, Image.Image):\n            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        \n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image\",\n                        \"image\": frame,\n                    },\n                    {\"type\": \"text\", \"text\": prompt},\n                ],\n            }\n        ]\n        \n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        generated_ids = self.model.generate(**inputs, max_new_tokens=128)\n        generated_ids_trimmed = [\n            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        output_text = self.processor.batch_decode(\n            generated_ids_trimmed, \n            skip_special_tokens=True, \n            clean_up_tokenization_spaces=False\n        )\n        \n        return output_text[0]\n\n    def process_video(self, video_path, prompt, frame_interval=30):\n        \"\"\"Process video with adaptive analysis based on prompt type\"\"\"\n        # Detect prompt type\n        needs_timestamps = any(keyword in prompt.lower() for keyword in \n                             ['timestamp', 'when', 'time', 'moment', 'detect'])\n        \n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(\"Error opening video file\")\n        \n        frames = []\n        timestamps = []\n        frame_analyses = []\n        frame_count = 0\n        \n        print(\"Processing video frames...\")\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            if frame_count % frame_interval == 0:\n                timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n                frames.append(frame)\n                timestamps.append(timestamp)\n                print(f\"Captured frame at {timestamp:.2f}s\")\n            \n            frame_count += 1\n        \n        cap.release()\n        \n        # Process frames\n        for frame, timestamp in zip(frames, timestamps):\n            analysis = self.analyze_frame(frame, prompt)\n            frame_analyses.append({\n                'timestamp': timestamp,\n                'analysis': analysis\n            })\n            print(f\"Processed frame at {timestamp:.2f}s\")\n        \n        # Generate final output based on prompt type\n        if needs_timestamps:\n            # Return individual frame analyses for timestamp-specific queries\n            return frame_analyses\n        else:\n            # Aggregate analysis for general summaries\n            all_observations = [analysis['analysis'] for analysis in frame_analyses]\n            summary_prompt = f\"Based on these observations from different timestamps of the video, provide a comprehensive summary: {all_observations}\"\n            \n            # Use the first frame as a reference frame for the summary\n            if frames:\n                final_summary = self.analyze_frame(frames[0], summary_prompt)\n                return [{\n                    'timestamp': 'Full Video',\n                    'analysis': final_summary\n                }]\n        \n        return []\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T03:59:34.288382Z","iopub.execute_input":"2025-02-11T03:59:34.288987Z","iopub.status.idle":"2025-02-11T03:59:34.303351Z","shell.execute_reply.started":"2025-02-11T03:59:34.288960Z","shell.execute_reply":"2025-02-11T03:59:34.302279Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def main():\n    print(\"GPU available:\", torch.cuda.is_available())\n    if torch.cuda.is_available():\n        print(\"GPU name:\", torch.cuda.get_device_name(0))\n    \n    video_path = \"/kaggle/input/mydata/Two Wheeler Safety.mp4\"\n    prompt = input(\"Enter your prompt for video analysis: \")\n    \n    analyzer = VideoAnalyzer(model_size=\"7b\")\n    results = analyzer.process_video(video_path, prompt)\n    \n    # Print results\n    for result in results:\n        if result['timestamp'] == 'Full Video':\n            print(\"\\nComplete Video Analysis:\")\n        else:\n            print(f\"\\nTimestamp: {result['timestamp']:.2f}s\")\n        print(f\"Analysis: {result['analysis']}\")\n        print(\"-\" * 50)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T04:02:48.041811Z","iopub.execute_input":"2025-02-11T04:02:48.042152Z","iopub.status.idle":"2025-02-11T04:16:58.838847Z","shell.execute_reply.started":"2025-02-11T04:02:48.042126Z","shell.execute_reply":"2025-02-11T04:16:58.837905Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nGPU name: Tesla P100-PCIE-16GB\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your prompt for video analysis:  How many bikers are riding their bike without helmet? If possible can you give me the timestamp\n"},{"name":"stdout","text":"Loading model from: /kaggle/input/qwen2-vl/transformers/7b-instruct/1\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:1593: UserWarning: Current model requires 268437504 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f961f06986b427dabb0dd9ab8482375"}},"metadata":{}},{"name":"stdout","text":"Processing video frames...\nCaptured frame at 0.00s\nCaptured frame at 1.20s\nCaptured frame at 2.40s\nCaptured frame at 3.60s\nCaptured frame at 4.80s\nCaptured frame at 6.00s\nCaptured frame at 7.20s\nCaptured frame at 8.40s\nCaptured frame at 9.60s\nCaptured frame at 10.80s\nCaptured frame at 12.00s\nCaptured frame at 13.20s\nCaptured frame at 14.40s\nCaptured frame at 15.60s\nCaptured frame at 16.80s\nCaptured frame at 18.00s\nCaptured frame at 19.20s\nCaptured frame at 20.40s\nCaptured frame at 21.60s\nCaptured frame at 22.80s\nCaptured frame at 24.00s\nProcessed frame at 0.00s\nProcessed frame at 1.20s\nProcessed frame at 2.40s\nProcessed frame at 3.60s\nProcessed frame at 4.80s\nProcessed frame at 6.00s\nProcessed frame at 7.20s\nProcessed frame at 8.40s\nProcessed frame at 9.60s\nProcessed frame at 10.80s\nProcessed frame at 12.00s\nProcessed frame at 13.20s\nProcessed frame at 14.40s\nProcessed frame at 15.60s\nProcessed frame at 16.80s\nProcessed frame at 18.00s\nProcessed frame at 19.20s\nProcessed frame at 20.40s\nProcessed frame at 21.60s\nProcessed frame at 22.80s\nProcessed frame at 24.00s\n\nTimestamp: 0.00s\nAnalysis: There are no bikers riding their bike without helmets in the image.\n--------------------------------------------------\n\nTimestamp: 1.20s\nAnalysis: In the image, there are two bikers riding their bikes without helmets. The timestamp for this image is not provided in the image itself.\n--------------------------------------------------\n\nTimestamp: 2.40s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 3.60s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 4.80s\nAnalysis: There are no bikers riding their bike without helmets in the image.\n--------------------------------------------------\n\nTimestamp: 6.00s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 7.20s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 8.40s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 9.60s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 10.80s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 12.00s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 13.20s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 14.40s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 15.60s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 16.80s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 18.00s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 19.20s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 20.40s\nAnalysis: There are no bikers visible in the image.\n--------------------------------------------------\n\nTimestamp: 21.60s\nAnalysis: There is one biker riding without a helmet in the image. The timestamp for this biker is approximately 0.5 seconds.\n--------------------------------------------------\n\nTimestamp: 22.80s\nAnalysis: The image shows a biker riding a scooter without a helmet. The timestamp for this image is 12:30 PM.\n--------------------------------------------------\n\nTimestamp: 24.00s\nAnalysis: There are two people riding the motorcycle, and neither of them is wearing a helmet. The timestamp for this image is approximately 0.5 seconds.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}